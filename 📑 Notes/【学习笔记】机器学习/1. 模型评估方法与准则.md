## 模型评估的目标

模型评估的目标是选出泛化能力强的模型取完成机器学习的任务。
泛化能力强的模型能够很好的适用于未知的样本。在机器学习任务中，我们希望最终能够得到能够准确预测未知标签的样本、泛化能力强的模型。但是我们无法在模型评估阶段提前获取未来需要预测的未知样本，因此我们需要基于已有的数据进行切分来完成模型的训练和评估。通过切分出的数据去判断模型的状态（过拟合、欠拟合），从而进行进一步的迭代优化。
为了实现有效的模型评估，我们需要一套系统的 **评估方法** 和 **性能指标**。
- **评估方法**：为了保证客观评估模型，对数据集进行有效划分的实验方法。
- **性能指标**：量化度量模型效果的指标。

## 模型评估方法

模型评估的实验方法可以分为 **离线实验方法** 和 **在线实验方法**。
### 离线实验方法

离线实验方法是基于历史数据对模型进行实验验证和离线评估，最终选择一个较好的模型。
常用的离线实验方法包括：
- [[留出法]]（Hold-out）：从训练数据中保留出验证样本集，这部分数据不用于训练，仅用于模型评估。
- [[交叉验证法]]（Cross validation）：将数据进行多个分组，分别作为训练集和测试集进行多次训练并取平均，从而减少模型方差，增加稳定性。常用的分成 K 组数据的方法又称为K折交叉验证（K-Fold）。
- [[自助法]]（Bootstrap）：有时数据量比较少的场景，无法通过已有的数据来估计数据的整体分布，此时可以使用自助法有放回的抽样生成大量的伪样本，对伪样本进行计算从而获得统计量的分布，估计数据的整体分布。
### 在线实验方法

在线实验方法是通过基于真实数据对模型进行在线评估，并选择较好的模型。
在线实验方法通常使用 A/B Test 的方式进行，即针对在线的真实数据，使用多个不同待选模型进行预测，依据一定的性能指标，选择出表现更好的模型作为最终的线上使用。

## 模型评估指标

离线实验方法和在线实验方法的指标选择往往也不相同。离线方法由于面对的是固定、明确的标签数据，因此往往选择具体的面向数据类型的性能指标，例如准确率、查准率、召回率、ROC、AUC、PRC 等；在线评估则更侧重业务表现的评价指标，例如用户生命周期值、广告点击率、用户流失率等。由于不同的业务场景下对应的业务指标也不尽相同，本文仅针对常见的数据类型的性能指标进行展开。
面向数据的模型评估指标又可以进一步按照分类问题和回归问题进行划分。

### 回归问题常用性能指标

在回归问题中，我们会得到连续值的预测结果，然后对比标准答案，我们可以使用 MAE、MSE、RMSE 等评估指标来衡量预测结果相较于实际情况的偏离程度。偏离程度越小，说明回归模型的预测越准，模型性能越好。

- **平均绝对误差**（MAE）：又叫平均绝对离差，是所有标签值和模型回归预测值偏差的绝对值的平均。该指标能够直观的反映出误差值和真实值的偏离大小，同时由于是取得绝对值，因此不会因为偏离存在正负而相互抵消。
- **平均绝对百分误差**（MAPE）：由于平均绝对误差得到的是一个绝对的评价值，但是无法通过这个绝对数值直观的得到模型拟合的优劣程度。模型的优劣必须要通过参考相对值的对比才能衡量效果，因此引入了平均绝对百分误差（MAPE）的概念。MAPE 是对 MAE 的一种改进，在 MAE 的基础上进一步考虑了误差绝对值和样本真实值之间的相对比例关系。
- **均方误差**（MSE）：MAE（或MAPE） 虽然可以直观的得到模型回归的误差值，但是由于评估本身采用了绝对值的方式，因此会导致函数不平滑（例如残差从正到负的时候函数上会出现折返的尖角），从而导致函数不可求导，这会对后续的模型优化（例如梯度下降法）的计算引入问题。因此，为了平滑函数尖角，考虑使用残差的平方替代绝对值，既避免了残差正负抵消的问题，同时保证了函数可导性。因此均方误差就是对预测值和真实值之间的残差平方项取平均。
- **均方根误差**（RMSE）：均方根误差也成为标准误差。由于均方误差对残差进行了平方处理，因此量纲也是样本数据的平方量级，所以为了衡量误差和真实值之间的关系，需要对其进行开方，从而划归到统一量纲进行比较。因此引入均方根误差（RMSE）。RMSE 即由 MSE 开方得到。
- **决定系数** R²：决定系数 R² 和上述的偏离指标不同，它体现的是目标值 y 的变化中有多少可以使用自变量 x 来表示，它是回归方程的拟合程度的体现。R² 接近 1，表示自变量能够表示绝大部分的目标值的变化。
- **校正决定系数**：由于决定系数会随着自变量的个数不断增加（即无脑增加新的自变量，不论是否和目标值的变化真的有关系，R² 一定不会变小），因此引入校正决定系数，在 R² 基础上考虑了自变量个数。

### 分类问题常用的性能指标